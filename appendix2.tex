\chapter{Grassman Calculus} 
\label{appendix2}
% Most of this chapter is from Lecture 10: Fermions in the Grassman formalism
\begin{defn}
	Let $V$ be a complex or real vector space.
	A \underline{Grassman number} is an element $x\in
	\bigwedge(V)$ in the exterior algebra of $V$.
	Given a choice of basis $\{\theta_i\}_{i=1}^{\infty}$ for $V$, a
	\underline{Grassman variable} is an element of the basis set.

	The Grassman numbers are typically equipped with a linear involution operator
	$*$ that extends complex conjugation in the following way: for all
	$\lambda,\mu\in\mathbb{C}$	
	\[
	(\lambda x + \mu y)^* = \lambda^* x^* + \mu^*y^*,	\qquad
	\theta^* = \theta, \qquad
	(\theta_1\theta_2\cdots\theta_k)^*=\theta_k\cdots\theta_2\theta_1
	\] 
	where $\theta,\theta_1,\ldots,\theta_k$ are in the basis set, and $x,y$ are
	Grassman numbers.
\end{defn}

It is standard to omit the wedge symbol $\wedge$ when writing a Grassman number.
The Grassman numbers have a $\mathbb{Z}_2$-grading given by $\bigwedge(V) =
\bigwedge_+(V) \oplus \bigwedge_-(V)$ which are subspaces generated by products
of an even (resp. odd) number of Grassman variables. Elements in
$\bigwedge_-(V)$ anti-commute, so are called a-numbers, while elements in 
$\bigwedge_+(V)$ commute, so are called c-numbers.

\section{Differentiation}
Let $A\in \bigwedge(V)$. For a given Grassman variable  $\theta_i$, after some
commutations we can uniquely write $A = A_1 + \theta_iA_2$ where $A_1,A_2$ do not
contain $\theta_i$. Then the differential operator with respect to $\theta_i$ is
defined as
\[
\pdv{A}{\theta_i} = A_2
\] 
In particular, we have $\pdv{\theta_j}{\theta_i} = \delta_{ij}$.

Note that the action of $\pdv{}{\theta_i}$ consists of commuting $\theta_i$ to
the left in all monomials before suppressing it. So the
derivaive of a product does not satisfy the usual Leibnitz rule.

Let $P$ be the algebra automorphism which satisfies $P(\theta_i)=-\theta_i$ for
each Grassman variable. Note that this relation uniquely determines  $P$,
which has the properties
$P(\theta_{i_1}\cdots\theta_{i_p})=(-1)^p\theta_{i_1}\cdots\theta_{i_p}$ and  
$A\theta_i = \theta_iP(A)$.

We find that the Leibnitz rule is replaced by
 \[
\pdv{}{\theta_i}(AB) = \pdv{A}{\theta_i}B + P(A) \pdv{B}{\theta_i} 
\] 
because if $A$ contains  $\theta_i$ then nothing in $B$ is commuted, but if
$B$ contains  $\theta_i$ then it needs to be commuted through all the variables
in $A$. This can be proved by simply expanding $A = A_1+\theta_iA_2$
and $B=B_1+\theta_iB_2$.

\section{Integration}
To define integration, we consider what properties it should have in relation to
differentiation. Let $D$ denote differentiation and  $I$ denote integration with
respect to the same Grassman variable. We would like the following relations
\begin{enumerate}[(1)]
	\item $I(A+B) = I(A)+I(B)$. Integration is linear 
    \item $ID = 0$. Integral of the derivative vanishes, a property that
		allows integration by parts
	 \item $DI = 0$. After integration over a variable, the result does not
		 depend on this variable any more.
	\item $D(A) = 0$ implies  $I(BA) = I(B)A$
\end{enumerate}
where $A,B$ are arbitrary Grassman numbers. If we are integrating with respect
to $\theta_i$,  and $A=A_1+\theta_iA_2$, then $I(A)=I(A_1)+I(\theta_i)A_2 =
I(\theta_i)A_2$ by linearity and property (4). Note that $I(A_1)=0$ because $A_1
= \pdv{}{\theta_i} (\theta_iA_1)$. Since $I(\theta_i)$ should not depend on
$\theta_i$ anymore, we are left to choose a constant in  $\mathbb{C}$.
If we set $I(\theta_i)=1$ for each Grassman variable, this is exactly the same
definition as the Grassman derivative!

\begin{defn}
	The \underline{Berezin integral} over the sole Grassman variable $\theta$ is
	an operator $\int d\theta : \bigwedge V \to \bigwedge V$. Given a Grassman
	number $A=A_1+\theta A_2$ where  $A_1,A_2$ do not depend on  $\theta$, define
	$\int A d\theta = \pdv{A}{\theta} = A_2$.
\end{defn}

We can extend the definition to integrate over any Grassman number in 
$\bigwedge V = T(V) /I$, by 
choosing a representative and integrating in that order. This is well defined
because the integral vanishes on the ideal $I$ generated by $\theta\otimes
\theta$ for $\theta\in V$. 
 
For example, if $\dim V = n$ and $\{\theta_i\}_{i=1}^n$ is a basis, the
Berezin integral over $\theta_1\wedge\ldots\wedge \theta_n$ takes the form 
$\int d\theta : \bigwedge V \to \mathbb{R}$, 
which kills any $\omega\in \bigwedge^k(V)$ with  $k<n$, and maps 
 $\theta_1\wedge \ldots\wedge\theta_n$ to 1.

\subsection{Integration on vector-valued differential forms} 
\label{section:berezin_forms}
If $E\xrightarrow{\pi} M$ is an oriented vector bundle, the Berezin integral
can be extended to a map on $\bigwedge E$-valued differential forms on a 
manifold $M$. We have a
non-vanishing section  $\nu\in \Gamma(M,\bigwedge^n E)$, and 
$\int^B : \Omega^*(M,\bigwedge E) \to \Omega^*(M)$ is defined at $x$ by
integrating over $\nu(x)\in \bigwedge^n E$. 

Another way to formulate this is in terms of a metric on $E$. The induced
metric on  $\bigwedge E$ given by 
\[
	\gen{v_1\wedge\ldots\wedge v_k, w_1\wedge\ldots\wedge w_l} 
	:= 
	\begin{cases}
		\det(\gen{v_i,w_j})	& k=l \\
		0 & k \neq l
	\end{cases}
\] 
Then the Berezin integral on $\bigwedge E$ valued forms can be defined at $x\in
M$ as $\int^B = \gen{\nu(x),-}$. Note that this is equivalent to integrating over 
$\nu(x)$. 

If in addition there is a metric compatible connection $\nabla$ on $E$, this 
induces a connection on $\bigwedge^n E$, and we choose the section
$\nu\in\Gamma(M,\bigwedge^n E)$ such that $\nabla \nu = 0$. 
To see why this can be done, recall that the induced connection is given by
\begin{multline*}
	\nabla_X(\sigma_1\wedge\sigma_2\wedge\cdots\wedge\sigma_n) 
	= \nabla_X \sigma_1
\wedge \sigma_2 \wedge \cdots \wedge \sigma_n + \sigma_1 \wedge \nabla_X\sigma_2
\wedge \sigma_3 \wedge \cdots \wedge \sigma_n + \cdots \\
+ \sigma_1 \wedge \cdots \wedge \sigma_{k-1}\wedge \nabla_X \sigma_n
\end{multline*}
This connection is compatible with the induced metric on $\bigwedge^nE$ because
to evaluate $D \gen{v_1\wedge\ldots\wedge v_n,w_1\wedge\ldots\wedge w_n}$, 
we can apply the product rule to each term in the determinant, and collect the
terms with $\nabla v_i$ and $\nabla w_j$ which gives the action of the induced
connection. Since $\bigwedge^n E$ is trivial, there is a global orthonormal
frame  $\nu=\sigma_1\wedge\ldots\wedge\sigma_n \in \Gamma(\bigwedge^nE)$. 
Moreover, $\gen{\nu,\nu}=1$ implies that $\nabla \nu = 0$ by metric
compatibility, which is what we wanted to show.

\section{Useful identities} \label{section:berezin_identities}
A single variable polynomial of degree $m$ of a Grassman number can be written 
in terms of its Taylor expansion as 
$p(x) = \sum_{k=0}^{m} \frac{p^{(k)}(a)}{k!} (x-a)^k$.
We are free to choose $a\in\mathbb{R}$ for every Grassman number $x$, because the
result still expresses the same polynomial. We would like to extend this for
smooth functions, but the choice of $a\in\mathbb{R}$ matters if we truncate the
series. 
\begin{defn}[Smooth function of Grassman number] \label{defn:grassman_function}
	% BGV Lemma 1.51 (2)
	Let $f\in C^\infty(\mathbb{R})$ and 
	$z \in \bigwedge(\theta_1,\ldots,\theta_n)$ be a Grassman number. 
	Denote $z_B \in \bigwedge^0 = \mathbb{R}$ to be the projection onto the 
	field. Then define $f(z)$ by
	\begin{equation}
	f(z) := \sum_{k=0}^{n} \frac{f^{(k)}(z_B)}{k!} (z-z_B)^k
	\end{equation}
\end{defn}
In other words, a smooth function of a Grassman number $z$ is defined to be its
Taylor series at $z_B$. A particular smooth function we will be interested in is
the exponential function $e^{x}$. In this case, the point at which the Taylor
expansion is taken does not matter because $z_B\in \mathbb{R}$ commutes with all
other numbers:
\begin{equation} \label{eq:grassman_exp}
e^{z}
=\sum_{k=0}^{n} \frac{e^{z_B}}{k!}(z-z_B)^k
= \paren{\sum_{k=0}^{\infty} \frac{1}{k!}z_B^k}\paren{\sum_{k=0}^{n} \frac{1}{k!}(z-z_B)^k}
= \sum_{k=0}^{\infty} \frac{1}{k!}z^k
\end{equation}
If $\theta=[\theta_1,\ldots,\theta_n]$ are Grassman variables then we can also
consider multivariable polynomials $f(\theta) \in
\bigwedge (\theta_1,\ldots,\theta_n)$. For example, if there are two arguments 
the function is of the form
$f(\theta_1,\theta_2)=A+B\theta_1+C\theta_2 + D\theta_1\theta_2$. 

\begin{lem} \label{lem:berezin_det}
	If $\theta = [\theta_1,\ldots,\theta_n]$ and $\eta=[\eta_1,\ldots,\eta_n]$
	are two lists of Grassman variables such that $\theta\cup\eta$ is linearly
	independent, and $K$ is a  $n\times n$ matrix,
	\[
	\int \exp (\theta^\intercal K \eta) 
	\odif{\theta_1}\odif{\eta_1}\cdots\odif{\theta_n}\odif{\eta_n} = \det K
	\] 
\end{lem}
\begin{proof}
	The terms from the series expansion of the exponential that give a
	non-vanishing contribution to the integral are those that contain the product
	$\theta_1\cdots\theta_n\eta_1\cdots\eta_n$, up to some permutation.
	This can only come from the term $\frac{1}{n!}(\theta_i K_{ij} \eta_j)^n$. Note that we
	can commute pairs of Grassman variables $\theta_{i_k} \eta_{j_k}$, so this gives
	\begin{align*}
	&\quad \frac{1}{n!}\sum_{i\in S_n} \sum_{j\in S_n} 
	K_{i_1j_1}K_{i_2j_2} \cdots K_{i_nj_n} 
	\theta_{i_1}\eta_{j_1}\cdots \theta_{i_n}\eta_{j_n} \\
	&= \sum_{j\in S_n} K_{1j_1}K_{2j_2} \cdots K_{nj_n} \theta_1\eta_{j_1}\cdots
	\theta_n\eta_{j_n} 
	\end{align*}
	After commuting the variables to cast each product into the order 
	$\theta_1\cdots\theta_n\eta_1\cdots\eta_{n}$, this introduces the sign
	$\operatorname{sgn}(j)$. Then after integrating each of the variables, we
	are left with the determinant. 
	\[
		\sum_{j\in S_n} \sgn(j) K_{1j_1}K_{2j_2} \cdots K_{nj_n} = \det(K)
	\] 
\end{proof}
\begin{lem} \label{lem:berezin_pf}
	If $\theta = [\theta_1,\ldots,\theta_n]$ are Grassman
	variables, and $M$ is a skew-symmetric  $n\times n$ matrix,
	\[
	\int \exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} d\theta = \Pf(M)
	\] 
\end{lem}
\begin{proof}
	The only non-vanishing terms in the exponential series are those that
	contain that product $\theta_1\theta_2\cdots\theta_n$ up to some
	permutation. But since all the terms contain an even number of Grassman
	variables, when  $n$ is odd the integral vanishes. Accordingly, the Pfaffian
	is zero for odd $n$.
	Now suppose  $n=2m$.
	Then non-vanishing terms come from
	$\frac{1}{2^mm!}(\theta_iM_{ij}\theta_j)^m$, where each permutation of
	$\theta_1\cdots\theta_n$ occurs once, so we are left with
	\begin{align*}
		\frac{1}{2^mm!}\sum_{j\in S_n} M_{j_1j_2}M_{j_3j_4}\cdots M_{j_{n-1}j_n}
		\theta_{j_1}\theta_{j_2}\cdots \theta_{j_{n-1}}\theta_{j_n}
	\end{align*}
	Commutating the variables to cast each product into a standard order
	introduces $\sgn(j)$, and integrating gives us the Pfaffian
	\[
	\Pf(M) = \frac{1}{2^{m}m!} \sum_{j\in S_n} \sgn(j)
	M_{j_1j_2}M_{j_3j_4}\cdots M_{j_{n-1}j_n}
	\] 
\end{proof}
\begin{remark}
	Sometimes the two lemmas above are written with a minus sign in the
	exponential. This is explained by the order of integration. For the
	Pfaffian, assuming $n$ even, we can alternatively write it as 
	\begin{align*}
		\Pf(M)
		&= \int \exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} d\theta \\
		&= \int \odif{\theta_n}\cdots \odif{\theta_1} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= (-1)^{\frac{n(n-1)}{2}}\int \odif{\theta_1}\cdots \odif{\theta_n} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= (-1)^{\frac{n}{2}}\int \odif{\theta_1}\cdots \odif{\theta_n} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= \int \odif{\theta} 
		\exp\paren{-\frac{1}{2}\theta^{\intercal}M\theta} 
	\end{align*}
	Note that the sign of the permutation $(1,\ldots,n)\mapsto(n,\ldots,1)$ is
	given by $(-1)^{\frac{n(n-1)}{2}}$, which is $(-1)^{\frac{n}{2}}$ when $n$ is even.
	Also the discussion still works for odd $n$, because the integral vanishes
	regardless.
\end{remark}

\begin{prop}[Invariance under translation {\cite[Prop A.12]{berezin_formulas}}] 
	\label{prop:berezin_translation}
	% prop A.12 berezin identities
	Let $\theta = [\theta_1,\ldots,\theta_n]$ be Grassman variables, and $\xi =
	[\xi_1,\ldots,\xi_n]$ be odd Grassman numbers that do not involve
	$\theta_i$. Then for any smooth function $f$, 
	\[
	\int f(\theta + \xi) \odif{\theta} = \int f(\theta) \odif{\theta}
	\] 
\end{prop}
\begin{proof}
	We can write $f(\theta)$ as a finite linear combination of elements of the
	form $\theta^J$, where  $J \subset [n]$. 
	Hence, it suffices to prove this for $f(\theta) =\theta^J$. 
	In this case, $f(\theta+\xi) = (\theta+\xi)^{J}$. 
	We have 
	\[
	\pdv{}{\theta_i}(\theta_j+\xi_j) = \pdv{}{\theta_i} \theta_j = \delta_{ij}
	\] 
	Therefore, in	
	$\pdv{}{\theta_n}\cdots\pdv{}{\theta_1}(\theta+\xi)^J$, each operator
	$\pdv{}{\theta_j}$ acts successively on $(\theta_j+\xi_j)$ to give an 
	element that is the same as if $\xi^j$ was set to zero. Hence, 
	 \[
	\pdv{}{\theta_n}\cdots\pdv{}{\theta_1}(\theta+\xi)^J = 
	\pdv{}{\theta_n}\cdots\pdv{}{\theta_1}\theta^J
	 \] 
\end{proof}

\begin{prop} \label{prop:berezin_formula} % MQformula 1.8
	If $\theta = [\theta_1,\ldots,\theta_n]$ are Grassman
	variables, $J= [J_1,\ldots,J_n]$ are odd Grassman numbers that do not
	involve $\theta_i$, and $M$ is an invertible skew-symmetric  $n\times n$ matrix,
	\[
	\int \exp\paren{\frac{1}{2}\theta^{\intercal}M\theta + J^\intercal\theta} d\theta =
	\Pf(M)\exp\paren{\frac{1}{2}J^\intercal M^{-1}J}
	\]
\end{prop}
\begin{proof}
	Since the variables in $\theta$ and $J$ anticommute, we can complete the
	square for  $\frac{1}{2}\theta^{\intercal}M\theta + J^\intercal\theta$ in
	the following way:
	\begin{align*}
		&\quad\frac{1}{2}(\theta-M^{-1}J)^\intercal M (\theta - M^{-1} J) + \frac{1}{2}
		J^\intercal M^{-1}J \\
		&=\frac{1}{2}(\theta^\intercal+J^\intercal M^{-1})M (\theta 
		- M^{-1} J) + \frac{1}{2} J^\intercal M^{-1}J \\
		&= \frac{1}{2}\theta^\intercal M \theta - \frac{1}{2} \theta^\intercal J
		+\frac{1}{2}J^\intercal \theta - \frac{1}{2}J^\intercal M^{-1} J +
		\frac{1}{2} J^\intercal M^{-1} J\\
		&= \frac{1}{2}\theta^\intercal M \theta + J^\intercal \theta 
	\end{align*}
	where we have used the fact that $(M^{-1})^\intercal = (M^\intercal)^{-1} =
	-M^{-1}$ and $\theta^\intercal J = - J^\intercal \theta$. 
	 
	Substituting $\chi = \theta - M^{-1}J$, the integral $\int
	\exp(\frac{1}{2}\theta^\intercal M\theta + J^\intercal\theta)\odif{\theta}$
	becomes
	\begin{align*}
	\int \exp\paren{\frac{1}{2}\chi^\intercal M \chi + \frac{1}{2}J^\intercal
	M^{-1} J}
	\odif{\theta}
	&= \int \exp\paren{\frac{1}{2}\chi^\intercal M \chi }
	\odif{\theta} \exp\paren{\frac{1}{2}J^\intercal M^{-1} J}	\\
	&= \int \exp\paren{\frac{1}{2}\theta^\intercal M \theta }
	\odif{\theta} \exp\paren{\frac{1}{2}J^\intercal M^{-1} J}\tag{by Prop
	\ref{prop:berezin_translation}}\\
	&= \Pf(M) \exp\paren{\frac{1}{2}J^\intercal M^{-1} J} \tag{by Lemma
	\ref{lem:berezin_pf}}
	\end{align*}
	Note that in general for $x,y\in\bigwedge V$, $\exp(x+y)=\exp(x)\exp(y)$
	does not hold. But it holds in this case because $J^\intercal M J$ is an even Grassman
	number. 	
\end{proof}
