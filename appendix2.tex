\chapter{Grassman Calculus} 
\label{appendix2}
% Most of this chapter is from Lecture 10: Fermions in the Grassman formalism
\begin{defn}
	Let $V$ be a complex or real vector space.
	A \underline{Grassman number} is an element $x\in
	\bigwedge(V)$ in the exterior algebra of $V$.
	Given a choice of basis $\{\theta_i\}_{i=1}^{\infty}$ for $V$, a
	\underline{Grassman variable} is an element of the basis set.

	The Grassman numbers are typically equipped with a linear involution operator
	$*$ that extends complex conjugation in the following way: for all
	$\lambda,\mu\in\mathbb{C}$	
	\[
	(\lambda x + \mu y) = \lambda^* x^* + \mu^*y^*,	\qquad
	\theta^* = \theta, \qquad
	(\theta_1\theta_2\cdots\theta_k)^*=\theta_k\cdots\theta_2\theta_1
	\] 
	where $\theta,\theta_1,\ldots,\theta_k$ are in the basis set, and $x,y$ are
	Grassman numbers.
\end{defn}

It is standard to omit the wedge symbol $\wedge$ when writing a Grassman number.
The Grassman numbers have a $\mathbb{Z}_2$-grading given by $\bigwedge(V) =
\bigwedge_+(V) \oplus \bigwedge_-(V)$ which are subspaces generated by products
of an even (resp. odd) number of Grassman variables. Elements in
$\bigwedge_-(V)$ anti-commute, so are called a-numbers, while elements in 
$\bigwedge_+(V)$ commute, so are called c-numbers.

\section{Differentiation}
Let $A\in \bigwedge(V)$. For a given Grassman variable  $\theta_i$, after some
commutations we can uniquely write $A = A_1 + \theta_iA_2$ where $A_1,A_2$ do not
contain $\theta_i$. Then the differential operator with respect to $\theta_i$ is
defined as
\[
\pdv{A}{\theta_i} = A_2
\] 
In particular, we have $\pdv{\theta_j}{\theta_i} = \delta_{ij}$.

Note that the action of $\pdv{}{\theta_i}$ consists of commuting $\theta_i$ to
the left in all monomials before suppressing it. So the
derivaive of a product does not satisfy the usual Leibnitz rule.

Let $P$ be the algebra automorphism which satisfies $P(\theta_i)=-\theta_i$ for
each Grassman variable. Note that this relation uniquely determines  $P$,
which has the properties
$P(\theta_{i_1}\cdots\theta_{i_p})=(-1)^p\theta_{i_1}\cdots\theta_{i_p}$ and  
$A\theta_i = \theta_iP(A)$.

We find that the Leibnitz rule is replaced by
 \[
\pdv{}{\theta_i}(AB) = \pdv{A}{\theta_i}B + P(A) \pdv{B}{\theta_i} 
\] 
because if $A$ contains  $\theta_i$ then nothing in $B$ is commuted, but if
$B$ contains  $\theta_i$ then it needs to be commuted through all the variables
in $A$. This can be proved by simply expanding $A = A_1+\theta_iA_2$
and $B=B_1+\theta_iB_2$.

\section{Integration}
To define integration, we consider what properties it should have in relation to
differentiation. Let $D$ denote differentiation and  $I$ denote integration with
respect to the same Grassman variable. We would like the following relations
\begin{enumerate}[(1)]
	\item $I(A+B) = I(A)+I(B)$. Integration is linear 
    \item $ID = 0$. Integral of the derivative vanishes, a property that
		allows integration by parts
	 \item $DI = 0$. After integration over a variable, the result does not
		 depend on this variable any more.
	\item $D(A) = 0$ implies  $I(BA) = I(B)A$
\end{enumerate}
where $A,B$ are arbitrary Grassman numbers. If we are integrating with respect
to $\theta_i$,  and $A=A_1+\theta_iA_2$, then $I(A)=I(A_1)+I(\theta_i)A_2 =
I(\theta_i)A_2$ by linearity and property (4). Note that $I(A_1)=0$ because $A_1
= \pdv{}{\theta_i} (\theta_iA_1)$. Since $I(\theta_i)$ should not depend on
$\theta_i$ anymore, we are left to choose a constant in  $\mathbb{C}$.
If we set $I(\theta_i)=1$ for each Grassman variable, this is exactly the same
definition as the Grassman derivative!

\begin{defn}
	The \underline{Berezin integral} over the sole Grassman variable $\theta$ is
	an operator $\int d\theta : \bigwedge V \to \bigwedge V$. Given a Grassman
	number $A=A_1+\theta A_2$ where  $A_1,A_2$ do not depend on  $\theta$, define
	$\int A d\theta = \pdv{A}{\theta} = A_2$.
\end{defn}

We can extend the definition to integrate over any Grassman number in 
$\bigwedge V = T(V) /I$, by 
choosing a representative and integrating in that order. This is well defined
because the integral vanishes on the ideal $I$ generated by $\theta\otimes
\theta$ for $\theta\in V$. 
 
For example, if $\dim V = n$ and $\{\theta_i\}_{i=1}^n$ is a basis, the
Berezin integral over $\theta_1\wedge\ldots\wedge \theta_n$ takes the form 
$\int d\theta : \bigwedge V \to \mathbb{R}$, 
which kills any $\omega\in \bigwedge^k(V)$ with  $k<n$, and maps 
 $\theta_1\wedge \ldots\wedge\theta_n$ to 1.

\subsection{Integration on vector-valued differential forms}
If $E\xrightarrow{\pi} M$ is an oriented vector bundle, the Berezin integral
can be extended to a map on $\bigwedge E$-valued differential forms on a 
manifold $M$. We have a
non-vanishing section  $\nu\in \Gamma(M,\bigwedge^n E)$, and 
$\int^B : \Omega^*(M,\bigwedge E) \to \Omega^*(M)$ is defined at $x$ by
integrating over $\nu(x)\in \bigwedge^n E$. 

Another way to formulate this is in terms of a metric on $E$. The induced
metric on  $\bigwedge E$ given by 
\[
	\gen{v_1\wedge\ldots\wedge v_k, w_1\wedge\ldots\wedge w_l} 
	:= 
	\begin{cases}
		\det(\gen{v_i,w_j})	& k=l \\
		0 & k \neq l
	\end{cases}
\] 
Then the Berezin integral on $\bigwedge E$ valued forms can be defined at $x\in
M$ as $\int^B = \gen{\nu(x),-}$. Note that this is equivalent to integrating over 
$\nu(x)$. 

If in addition there is a metric compatible connection $\nabla$ on $E$, this 
induces a connection on $\bigwedge^n E$, and we choose the section
$\nu\in\Gamma(M,\bigwedge^n E)$ such that $\nabla \nu = 0$. 
To see why this can be done, recall that the induced connection is given by
\begin{multline*}
	\nabla_X(\sigma_1\wedge\sigma_2\wedge\cdots\wedge\sigma_n) 
	= \nabla_X \sigma_1
\wedge \sigma_2 \wedge \cdots \wedge \sigma_n + \sigma_1 \wedge \nabla_X\sigma_2
\wedge \sigma_3 \wedge \cdots \wedge \sigma_n + \cdots \\
+ \sigma_1 \wedge \cdots \wedge \sigma_{k-1}\wedge \nabla_X \sigma_n
\end{multline*}
This connection is compatible with the induced metric on $\bigwedge^nE$ because
to evaluate $D \gen{v_1\wedge\ldots\wedge v_n,w_1\wedge\ldots\wedge w_n}$, 
we can apply the product rule to each term in the determinant, and collect the
terms with $\nabla v_i$ and $\nabla w_j$ which gives the action of the induced
connection. Since $\bigwedge^n E$ is trivial, there is a global orthonormal
frame  $\nu=\sigma_1\wedge\ldots\wedge\sigma_n \in \Gamma(\bigwedge^nE)$. 
Moreover, $\gen{\nu,\nu}=1$ implies that $\nabla \nu = 0$ by metric
compatibility, which is what we wanted to show.

\section{Useful identities}
A smooth function of Grassman variables is defined as the Taylor
series of the function. 
Let $f$ be a smooth function. Then the Taylor series  $f(\theta)=A+B\theta$  
terminates after two terms because  $\theta^2=0$. 
For instance, $e^{\theta} = 1 + \theta$. 

\begin{lem}
	If $\theta = [\theta_1,\ldots,\theta_n]$ and $\eta=[\eta_1,\ldots,\eta_n]$
	are two lists of Grassman variables such that $\theta\cup\eta$ is linearly
	independent, and $K$ is a  $n\times n$ matrix,
	\[
	\int \exp (\theta^\intercal K \eta) 
	\odif{\theta_1}\odif{\eta_1}\cdots\odif{\theta_n}\odif{\eta_n} = \det K
	\] 
\end{lem}
\begin{proof}
	The terms from the series expansion of the exponential that give a
	non-vanishing contribution to the integral are those that contain the product
	$\theta_1\cdots\theta_n\eta_1\cdots\eta_n$, up to some permutation.
	This can only come from the term $\frac{1}{n!}(\theta_i K_{ij} \eta_j)^n$. Note that we
	can commute pairs of Grassman variables $\theta_{i_k} \eta_{j_k}$, so this gives
	\begin{align*}
	&\quad \frac{1}{n!}\sum_{i\in S_n} \sum_{j\in S_n} 
	K_{i_1j_1}K_{i_2j_2} \cdots K_{i_nj_n} 
	\theta_{i_1}\eta_{j_1}\cdots \theta_{i_n}\eta_{j_n} \\
	&= \sum_{j\in S_n} K_{1j_1}K_{2j_2} \cdots K_{nj_n} \theta_1\eta_{j_1}\cdots
	\theta_n\eta_{j_n} 
	\end{align*}
	After commuting the variables to cast each product into the order 
	$\theta_1\cdots\theta_n\eta_1\cdots\eta_{n}$, this introduces the sign
	$\operatorname{sgn}(j)$. Then after integrating each of the variables, we
	are left with the determinant. 
	\[
		\sum_{j\in S_n} \sgn(j) K_{1j_1}K_{2j_2} \cdots K_{nj_n} = \det(K)
	\] 
\end{proof}
\begin{lem}
	If $\theta = [\theta_1,\ldots,\theta_n]$ are linearly independent Grassman
	variables, and $M$ is a skew-symmetric  $n\times n$ matrix,
	\[
	\int \exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} d\theta =
	\begin{cases}
		\Pf(M) & n \textrm{ even} \\
		0 & n \textrm{ odd}
	\end{cases}
	\] 
\end{lem}
\begin{proof}
	The only non-vanishing terms in the exponential series are those that
	contain that product $\theta_1\theta_2\cdots\theta_n$ up to some
	permutation. But since all the terms contain an even number of Grassman
	variables, when  $n$ is odd the integral vanishes. Now suppose  $n=2m$.
	Then non-vanishing terms come from
	$\frac{1}{2^mm!}(\theta_iM_{ij}\theta_j)^m$, where each permutation of
	$\theta_1\cdots\theta_n$ occurs once, so we are left with
	\begin{align*}
		\frac{1}{2^mm!}\sum_{j\in S_n} M_{j_1j_2}M_{j_3j_4}\cdots M_{j_{n-1}j_n}
		\theta_{j_1}\theta_{j_2}\cdots \theta_{j_{n-1}}\theta_{j_n}
	\end{align*}
	Commutating the variables to cast each product into a standard order
	introduces $\sgn(j)$, and integrating gives us the Pfaffian
	\[
	\Pf(M) = \frac{1}{2^{m}m!} \sum_{j\in S_n} \sgn(j)
	M_{j_1j_2}M_{j_3j_4}\cdots M_{j_{n-1}j_n}
	\] 
\end{proof}
\begin{remark}
	Sometimes the two lemmas above are written with a minus sign in the
	exponential. This is explained by the order of integration. For the
	Pfaffian, assuming $n$ even, we can alternatively write it as 
	\begin{align*}
		\Pf(M)
		&= \int \exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} d\theta \\
		&= \int \odif{\theta_n}\cdots \odif{\theta_1} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= (-1)^{\frac{n(n-1)}{2}}\int \odif{\theta_1}\cdots \odif{\theta_n} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= (-1)^{\frac{n}{2}}\int \odif{\theta_1}\cdots \odif{\theta_n} 
		\exp\paren{\frac{1}{2}\theta^{\intercal}M\theta} \\
		&= \int \odif{\theta} 
		\exp\paren{-\frac{1}{2}\theta^{\intercal}M\theta} 
	\end{align*}
	Note that the sign of the permutation $(1,\ldots,n)\mapsto(n,\ldots,1)$ is
	given by $(-1)^{\frac{n(n-1)}{2}}$, which is $(-1)^{\frac{n}{2}}$ when $n$ is even.
	Also the discussion still works for odd $n$, because the integral vanishes
	regardless.
\end{remark}
